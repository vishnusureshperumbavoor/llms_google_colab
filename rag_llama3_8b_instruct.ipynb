{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnusureshperumbavoor/rag_apps/blob/main/rag_llama3_8b_instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "_4Ja3q-y6MfF",
        "outputId": "17ce09a7-1d96-49c0-8689-0354345fdcef"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "# VSP's RAG app Llama3-8b-instruct ",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "display(Markdown(\"#VSP's RAG app Llama3-8b-instruct\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc6lM5m_6plj"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gTJ7qOKxtsYS"
      },
      "outputs": [],
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv\n",
        "!pip install llama-index==0.10.12\n",
        "!pip install -q gradio\n",
        "!pip install einops\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ORIw-sHut3iy"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-huggingface llama-index-embeddings-fastembed fastembed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K9Io5M90-MXs"
      },
      "outputs": [],
      "source": [
        "pip install transformers -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuiOFJtX6uFM"
      },
      "source": [
        "# Huggingface API import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8TRyczJOuaBp"
      },
      "outputs": [],
      "source": [
        "# add your huggingface API in colab secrets and allow access\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logging"
      ],
      "metadata": {
        "id": "TQAkfyrkOmkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "id": "FVjMaLr9OoxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEAGJTD-4eeu"
      },
      "source": [
        "# Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f86V5-tUuHGD"
      },
      "outputs": [],
      "source": [
        "# Process of breaking down a large input text into smaller pieces to improve retrieval efficiency. This ensures that the text fits the input size of the embedding model.\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# Create a folder called 'data' and upload the pdf into that folder\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models declaration"
      ],
      "metadata": {
        "id": "DrMSq957Owy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model=\"BAAI/bge-small-en-v1.5\"\n",
        "tokenizer_model=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "llm_model=\"meta-llama/Meta-Llama-3-8B-Instruct\""
      ],
      "metadata": {
        "id": "2Ro9Oui5OzOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aATZt63z4oxn"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FFzY62TuV3v"
      },
      "outputs": [],
      "source": [
        "# Technique for representing text data as numerical vectors, which can be input into ML models. The embedding model (FastEmbed) is responsible for converting text into numerical vectors.\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.embed_model = FastEmbedEmbedding(model_name=embedding_model)\n",
        "Settings.chunk_size = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mHjz7dI49iR"
      },
      "source": [
        "# Vector database (VectorStoreIndex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdkBB3ifugAD"
      },
      "outputs": [],
      "source": [
        "# Vector databases is used for fast retrieval and similarity search which can be used for CRUD operations, metadata filtering, and horizontal scaling.\n",
        "# By default, LlamaIndex uses VectorStoreIndex (simple in-memory vector store thatâ€™s great for quick experimentation)\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Create vector store and upload the indexed data\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiX39AQ_6Bmh"
      },
      "source": [
        "# Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q32cvI6buWtn"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index into LLM\n",
        "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FINzcdkfATow"
      },
      "source": [
        "# Tokenization (huggingface tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVpAwMCkAWfB"
      },
      "outputs": [],
      "source": [
        "# tiktoken\n",
        "# import tiktoken\n",
        "# Settings.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
        "\n",
        "# huggingface\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
        "\n",
        "stopping_ids = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSUfeIty-Jqg"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5zsXO8x-LAy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    context_window=8192,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=tokenizer_model,\n",
        "    model_name=llm_model,\n",
        "    device_map=\"auto\",\n",
        "    stopping_ids=stopping_ids,\n",
        "    tokenizer_kwargs={\"max_length\": 4096},\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")\n",
        "\n",
        "Settings.chunk_size = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LedNZoW95lmf"
      },
      "source": [
        "# Query engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27g3Y4jtudqi"
      },
      "outputs": [],
      "source": [
        "# The query engine takes query string to use it to fetch relevant context and then sends them both as a prompt to the LLM to generate a final natural language response.\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX2MinPx5zGD"
      },
      "source": [
        "# User Interface (gradio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oht7oYHzum0E"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def predict(input, history):\n",
        "  response = query_engine.query(input)\n",
        "  return str(response)\n",
        "\n",
        "gr.ChatInterface(predict).launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN4Vgg88AFQHFzw+MpKMBLy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}